
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4. Q-learning &#8212; 強化学習新人ゼミ2022</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="3. ベルマン方程式" href="03bellman.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">強化学習新人ゼミ2022</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="この本を検索..." aria-label="この本を検索..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   はじめに
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Day1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01what_is_rl.html">
   1. 強化学習が対象とする問題
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02value_function.html">
   2. 価値関数
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03bellman.html">
   3. ベルマン方程式
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   4. Q-learning
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="ナビゲーションを切り替え" aria-controls="site-navigation"
                title="ナビゲーションを切り替え" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="このページをダウンロード"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/day1/04q-learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="ソースファイルをダウンロード" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="PDFに印刷"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="ソースリポジトリ"><i
                    class="fab fa-github"></i>リポジトリ</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fday1/04q-learning.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="問題を開く"><i class="fas fa-lightbulb"></i>未解決の問題</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="全画面モード"
        title="全画面モード"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/day1/04q-learning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="発売 Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> 目次
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   4.1. これまでのまとめ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   4.2. Q-learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     4.2.1. 行動価値関数の更新
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     4.2.2. 行動の選択
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     4.2.3. まとめ
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   4.3. Q-learningの実装
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Q-learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> 目次 </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   4.1. これまでのまとめ
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   4.2. Q-learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     4.2.1. 行動価値関数の更新
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     4.2.2. 行動の選択
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     4.2.3. まとめ
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id6">
   4.3. Q-learningの実装
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="q-learning">
<h1><span class="section-number">4. </span>Q-learning<a class="headerlink" href="#q-learning" title="Permalink to this headline">¶</a></h1>
<div class="math notranslate nohighlight">
\[\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\E}{{\mathrm E}}
\newcommand{\underE}[2]{\underset{\begin{subarray}{c}#1 \end{subarray}}{\E}\left[ #2 \right]}
\newcommand{\Epi}[1]{\underset{\begin{subarray}{c}\tau \sim \pi \end{subarray}}{\E}\left[ #1 \right]}\]</div>
<div class="section" id="id1">
<h2><span class="section-number">4.1. </span>これまでのまとめ<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>これまで，モンテカルロ法による価値推定手法/ベルマン方程式を基にした価値関数の求解/ベルマン最適方程式を基にした最適方策の求解(価値反復/方策反復)，などを扱ってきました．</p>
<p>まず，モンテカルロ法に関して，本書で紹介した手法は価値推定のみですが，これを方策反復などと組み合わせることにより，最適方策を推定する手法を導くことも出来ます．
モンテカルロ法をもとにした手法の大きな特徴は，環境の情報を必要としないことです．
ベルマン方程式を基にした手法では状態遷移関数<span class="math notranslate nohighlight">\(p(s'|s,a)\)</span>や報酬関数<span class="math notranslate nohighlight">\(r(s,a)\)</span>などの情報が必要だったのに対して，モンテカルロ法はエージェントがサンプルした経験のみを必要とします．
しかし，この手法は，値を更新するために最低でもエピソード 1 つ分の経験を必要とします．
つまり，エピソード長が長い/無限に続くタスクに対しては適さない手法です．
また，モンテカルロ法は推定バイアスは無いものの，推定の分散が大きいことが知られています．</p>
<p>そこで本章では，
エージェントがサンプルした経験のみで学習でき，
エピソードが終了せずとも値を更新できる手法 <strong>Q-leraning</strong> を紹介します．</p>
</div>
<div class="section" id="id2">
<h2><span class="section-number">4.2. </span>Q-learning<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>Q-learning の大きなコンセプトは，</p>
<ul class="simple">
<li><p>エージェントが収集した経験をもとに</p></li>
<li><p>ベルマン方程式を解く</p></li>
</ul>
<p>というものです．エージェントが収集した経験をもとにするところはモンテカルロ法と共通ですが，ベルマン方程式を解くというコンセプトは価値反復/方策反復と共通です．(Q-learning は特に価値反復と関わりが深い)</p>
<p>まず，Q-learning の大まかな流れは以下のようになります．</p>
<div class="proof algorithm admonition" id="q-learning-concept">
<p class="admonition-title"><span class="caption-number">Algorithm 4.1 </span> (Q-learningの概要)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Inputs</strong> <span class="math notranslate nohighlight">\(q_0\)</span></p>
<p><strong>Output</strong> <span class="math notranslate nohighlight">\(\mu(s) = \argmax_{a} q(s,a)\)</span></p>
<ol class="simple">
<li><p>行動価値関数を初期化 <span class="math notranslate nohighlight">\(q \leftarrow q_0\)</span></p></li>
<li><p>while <em>still time to train</em>:</p>
<ol class="simple">
<li><p>行動<span class="math notranslate nohighlight">\(A_t\)</span>を<em>何かしらの方法</em> で選択．</p></li>
<li><p>行動<span class="math notranslate nohighlight">\(A_t\)</span>を実行して，経験<span class="math notranslate nohighlight">\((S_t,A_t,S_{t+1},R_t)\)</span>を得る．</p></li>
<li><p>経験<span class="math notranslate nohighlight">\((S_t,A_t,S_{t+1},R_t)\)</span>をもとに，<em>何かしらの方法</em> でベルマン方程式の解に近づくように<span class="math notranslate nohighlight">\(q\)</span>を更新．</p></li>
</ol>
</li>
</ol>
</div>
</div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Q-learningでは価値反復と違い，行動価値関数を推定しています．
価値反復では環境の全ての情報を用いることが出来たため，<span class="math notranslate nohighlight">\(V\)</span>から<span class="math notranslate nohighlight">\(Q\)</span>を求めることが出来たのに対し，本手法では環境の情報を用いないことを前提としているため，直接行動価値関数を求める必要があるためです．
行動価値関数も価値関数と同様にベルマン最適作用素<span class="math notranslate nohighlight">\(\Upsilon_*\)</span></p>
<div class="math notranslate nohighlight">
\[\Upsilon_*(q)(s,a) = r(s,a) + \gamma \sum_{s' \in \mathcal{S}} p(s'|s,a) \max_{a' \in \mathcal{A}} q(s',a') , \forall s \in \mathcal{S}, a \in \mathcal{A}\]</div>
<p>が存在します．これは，価値関数と同様，唯一の不動点が最適行動価値関数<span class="math notranslate nohighlight">\(Q^*\)</span>となり，作用素を繰り返し適用することにより任意の関数を<span class="math notranslate nohighlight">\(Q^*\)</span>に漸近的に近づけることが出来ます．</p>
</div>
<p>このような流れの手法が実現出来れば，上記のコンセプトを満たすことが出来ます．
ここで，このアルゴリズムで考えるべきポイントは以下の2点あります．</p>
<ol class="simple">
<li><p>経験<span class="math notranslate nohighlight">\((S_t,A_t,S_{t+1},R_t)\)</span>から，どうやって<span class="math notranslate nohighlight">\(q\)</span>を更新するか．</p></li>
<li><p>行動<span class="math notranslate nohighlight">\(A_t\)</span>をどのように選択するのか．</p></li>
</ol>
<div class="section" id="id3">
<h3><span class="section-number">4.2.1. </span>行動価値関数の更新<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>まず，経験<span class="math notranslate nohighlight">\((S_t,A_t,S_{t+1},R_t)\)</span>から，どうやって<span class="math notranslate nohighlight">\(q\)</span>を更新するかを考えます．</p>
<p>環境の完全な情報があれば，<span class="math notranslate nohighlight">\(q\)</span>は全ての状態/行動に対して，</p>
<div class="math notranslate nohighlight">
\[q \leftarrow \Upsilon_*(q) = r(s,a) + \gamma \sum_{s' \in \mathcal{S}} p(s'|s,a) \max_{a' \in \mathcal{A}} q(s',a') , \forall s \in \mathcal{S}, a \in \mathcal{A}\]</div>
<p>と更新すれば良いのですが，Q-learningで使える情報は<span class="math notranslate nohighlight">\((S_t,A_t,S_{t+1},R_t)\)</span>のみです．
価値反復では全ての状態行動に対して<span class="math notranslate nohighlight">\(q\)</span>を更新出来ましたが，<span class="math notranslate nohighlight">\((S_t,A_t,S_{t+1},R_t)\)</span>以外の情報が使えないと，
更新できる状態行動は<span class="math notranslate nohighlight">\(s = S_t, a = A_t\)</span>のときの行動価値<span class="math notranslate nohighlight">\(q(S_t, A_t)\)</span>のみとなります．
また，<span class="math notranslate nohighlight">\(R_t = r(S_t,A_t)\)</span>となりますが，<span class="math notranslate nohighlight">\(p(s'|S_t,A_t)\)</span>がわからないので，以下のように近似します．</p>
<div class="math notranslate nohighlight" id="equation-q-approx">
<span class="eqno">(4.1)<a class="headerlink" href="#equation-q-approx" title="Permalink to this equation">¶</a></span>\[
\sum_{s' \in \mathcal{S}} p(s'|S_t,A_t) \max_{a' \in \mathcal{A}} q(s',a') \simeq \max_{a' \in \mathcal{A}} q(S_{t+1}, a'), \ S_{t+1} \sim p(\cdot|S_t,A_t)
\]</div>
<p>これらより，</p>
<div class="math notranslate nohighlight" id="equation-q-update-1">
<span class="eqno">(4.2)<a class="headerlink" href="#equation-q-update-1" title="Permalink to this equation">¶</a></span>\[\begin{split}
q(S_t, A_t) &amp;\leftarrow R_t + \gamma \max_{a' \in \mathcal{A}} q(S_{t+1}, a')\\
q(s, a) &amp;\leftarrow q(s, a), \ s \neq S_t, a \neq A_t
\end{split}\]</div>
<p>という更新が考えられますが，これだけだとまだ不十分です．
<span class="math notranslate nohighlight">\(q(S_t, A_t)\)</span>の更新では式<a class="reference internal" href="#equation-q-approx">(4.1)</a>の近似誤差
<span class="math notranslate nohighlight">\(\max_{a' \in \mathcal{A}} q(S_{t+1}, a') - \sum_{s' \in \mathcal{S}} p(s'|S_t,A_t) \max_{a' \in \mathcal{A}} q(s',a') \)</span>が存在します．
式<a class="reference internal" href="#equation-q-update-1">(4.2)</a>の更新では，<span class="math notranslate nohighlight">\(q\)</span>を何度更新し続けても，この誤差の影響で値が振動的になってしまいます．
ここで，時刻<span class="math notranslate nohighlight">\(t\)</span>までに，ある状態行動対<span class="math notranslate nohighlight">\((s,a)\)</span>に対して，qを更新した回数を<span class="math notranslate nohighlight">\(n(t,s,a)\)</span>とおきます．
式<a class="reference internal" href="#equation-q-approx">(4.1)</a>のような近似を用いると，常にノイズは存在しますが，<span class="math notranslate nohighlight">\(n(S_t, A_t)\)</span>が十分に大きくなるにつれ，その影響を小さくするように更新することが出来ます．</p>
<div class="math notranslate nohighlight" id="equation-q-update-2">
<span class="eqno">(4.3)<a class="headerlink" href="#equation-q-update-2" title="Permalink to this equation">¶</a></span>\[\begin{split}
q(S_t, A_t) &amp;\leftarrow q(S_t, A_t) + \alpha(n(t, S_t, A_t))( R_t + \gamma \max_{a' \in \mathcal{A}} q(S_{t+1}, a') - q(S_t, A_t))\\
q(s, a) &amp;\leftarrow q(s, a), \ s \neq S_t, a \neq A_t
\end{split}\]</div>
<p>ここで<span class="math notranslate nohighlight">\(\alpha(x)\)</span>はステップサイズと呼ばれ，<span class="math notranslate nohighlight">\(x\)</span>が大きくなるにつれ，値が適切に減少していく関数です．
一般的に<span class="math notranslate nohighlight">\(\alpha(n(t, S_t, A_t)) = \frac{1}{n(t, S_t, A_t)}\)</span>などがよく用いられます．</p>
<p>式<a class="reference internal" href="#equation-q-update-2">(4.3)</a>こそがQ-learningにおける<span class="math notranslate nohighlight">\(q\)</span>の更新方法となります．</p>
<div class="admonition-q-learning admonition" id="q-learning-convergence">
<p class="admonition-title">Q-learningの収束</p>
<p>本節では，<span class="math notranslate nohighlight">\(q\)</span>の更新法を直感的に説明しましたが，</p>
<div class="math notranslate nohighlight">
\[\sum_{t} \alpha(n(t,s,a)) = \infty, \ \ \sum_{t} \alpha(n(t,s,a)) &lt; \infty, \ \  \forall s \in \mathcal{S}, a \in \mathcal{A}\]</div>
<p>という仮定を満たせば<span class="math notranslate nohighlight">\(q\)</span>は<span class="math notranslate nohighlight">\(Q^*\)</span>に収束することが証明されています．
詳しくは<a class="reference external" href="http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf">この資料</a>などを見てみてください．</p>
</div>
</div>
<div class="section" id="id4">
<h3><span class="section-number">4.2.2. </span>行動の選択<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>つぎに，行動の選択手法を考えていきます．</p>
<p>前節の行動価値関数の更新法から，<a class="reference internal" href="#q-learning-convergence"><span class="std std-ref">Q-learningの収束</span></a>の仮定を満たすような行動，つまり全状態行動対に満遍なく訪れるような行動を選択すれば良いのですが，効率よく学習をするためには，以下の2つが重要です．</p>
<ul class="simple">
<li><p><strong>探索(exploration)</strong></p></li>
<li><p><strong>活用(exploitation)</strong></p></li>
</ul>
<p>「探索」とは現在の行動価値で敢えて最良とされていない行動を選択することにより，情報量の高い経験を収集することを意味し，
「活用」とは現在の行動価値で最良とされている行動を選択することにより，高い報酬を得られる経験を収集することを意味します．</p>
<p>この２つはバランスが大事です．
例えば「探索」を怠る，つまり，行動が常に現在の<span class="math notranslate nohighlight">\(q\)</span>の値を最大化するような行動を選択すると，
エージェントが何回行動しても同じ様な経験しか得られず，
全状態行動対に満遍なく訪れることが出来なくなってしまいます．
ただ，Q-learningはローカルサーチ的な側面があるため，
現在最良であると思われる行動を選択することにより得られる経験を多く収集する，
つまり「活用」により得られる経験は学習効率の面で重要です．</p>
<p>そこで，この探索と活用のバランスを取った方策として<span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy方策というものがあります．
<span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy方策は，現在の行動価値関数の推定値<span class="math notranslate nohighlight">\(q\)</span>に対して以下のように行動を選択する方策です．</p>
<div class="math notranslate nohighlight">
\[\begin{split}\pi_{\epsilon\text{-greedy}}(a|s;q,\epsilon) = \left\{
\begin{array}{ll}
1-\epsilon + \frac{\epsilon}{|\mathcal{A}|}  &amp; (a = \argmax_{a'} q(s,a')) \\
\frac{\epsilon}{|\mathcal{A}|}  &amp; (それ以外)
\end{array}
\right.\end{split}\]</div>
<p>この方策は確率<span class="math notranslate nohighlight">\(\epsilon\)</span>で一様ランダムに行動し，それ以外は最良の行動を選択します．
ここで<span class="math notranslate nohighlight">\(\epsilon\)</span>はハイパーパラメータで，1に近いと一様ランダムに行動，つまり探索が重視され，0に近いと最良の行動，つまり活用が重視されます．</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>じつは<span class="math notranslate nohighlight">\(\epsilon\)</span>は定数である必要はなく，時間が経つにつれ小さくするなどといったことも可能です．</p>
</div>
</div>
<div class="section" id="id5">
<h3><span class="section-number">4.2.3. </span>まとめ<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>以上より，<a class="reference internal" href="#q-learning-concept">Algorithm 4.1</a>を完全な形にすると，以下のようになります．</p>
<div class="proof algorithm admonition" id="q-learning">
<p class="admonition-title"><span class="caption-number">Algorithm 4.2 </span> (Q-learning)</p>
<div class="algorithm-content section" id="proof-content">
<p><strong>Inputs</strong> <span class="math notranslate nohighlight">\(q_0, \epsilon\)</span></p>
<p><strong>Output</strong> <span class="math notranslate nohighlight">\(\mu(s) = \argmax_{a} q(s,a)\)</span></p>
<ol>
<li><p>行動価値関数を初期化 <span class="math notranslate nohighlight">\(q \leftarrow q_0\)</span></p>
<p>状態<span class="math notranslate nohighlight">\((s,a)\)</span>への訪問回数を初期化 <span class="math notranslate nohighlight">\(n(s,a) \leftarrow 0\)</span></p>
</li>
<li><p>while <em>still time to train</em>:</p>
<ol>
<li><p>行動<span class="math notranslate nohighlight">\(A_t \sim \pi_{\epsilon\text{-greedy}}(a|s;q,\epsilon)\)</span>を選択．</p></li>
<li><p>行動<span class="math notranslate nohighlight">\(A_t\)</span>を実行して，経験<span class="math notranslate nohighlight">\((S_t,A_t,S_{t+1},R_t)\)</span>を得る．</p>
<p>訪問回数を更新 <span class="math notranslate nohighlight">\(n(S_t, A_t) \leftarrow n(S_t, A_t) + 1\)</span></p>
</li>
<li><p><span class="math notranslate nohighlight">\(q\)</span>を更新</p>
<div class="math notranslate nohighlight">
\[\begin{split}q(S_t, A_t) &amp;\leftarrow q(S_t, A_t) + \alpha(n(S_t, A_t))( R_t + \gamma \max_{a' \in \mathcal{A}} q(S_{t+1}, a') - q(S_t, A_t))\\
          q(s, a) &amp;\leftarrow q(s, a), \ s \neq S_t, a \neq A_t\end{split}\]</div>
</li>
</ol>
</li>
</ol>
</div>
</div></div>
</div>
<div class="section" id="id6">
<h2><span class="section-number">4.3. </span>Q-learningの実装<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h2>
<p>実際に<a class="reference external" href="https://www.gymlibrary.ml/environments/toy_text/frozen_lake/">FrozenLake</a>環境に対してQ-learningを適用してみましょう．</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span>
    <span class="s2">&quot;FrozenLake-v1&quot;</span><span class="p">,</span>
    <span class="n">desc</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;SFFF&quot;</span><span class="p">,</span> <span class="s2">&quot;FHFH&quot;</span><span class="p">,</span> <span class="s2">&quot;FFFH&quot;</span><span class="p">,</span> <span class="s2">&quot;HFFG&quot;</span><span class="p">],</span>
    <span class="n">map_name</span><span class="o">=</span><span class="s2">&quot;4x4&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>まずは各パラメータを設定し，変数を初期化します．</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 再現性のためseedを固定</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>  <span class="c1"># 割引率</span>
<span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">0.4</span>  <span class="c1"># ε-greedy方策のパラメータ</span>
<span class="n">state_size</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span>  <span class="c1"># 状態数</span>
<span class="n">action_size</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>  <span class="c1"># 行動数</span>

<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">))</span>  <span class="c1"># 行動価値関数の初期値</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>次に<span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy方策を定義します．</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_eps_greedy</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>  <span class="c1"># 確率εでランダムな行動を選択</span>
        <span class="k">return</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># それ以外では最良の行動を選択</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="p">:])</span>
</pre></div>
</div>
</div>
</div>
<p>また，現在の<span class="math notranslate nohighlight">\(q\)</span>に対して最良の行動を選択した場合得られる報酬の期待値をモンテカルロ法でサンプルする関数<code class="docutils literal notranslate"><span class="pre">check_performance</span></code>を実装します．
この関数を学習途中で用いることにより，学習の進捗を確認します．</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_performance</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">):</span>  <span class="c1"># 50エピソード走らせる</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">reward_sum</span> <span class="o">=</span> <span class="mf">0.0</span>  <span class="c1"># 報酬の合計</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">policy_eps_greedy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>  <span class="c1"># 最良の行動を選択</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">reward_sum</span> <span class="o">+=</span> <span class="n">reward</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward_sum</span><span class="p">)</span>
                <span class="k">break</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>  <span class="c1"># 報酬の平均を求める</span>
</pre></div>
</div>
</div>
</div>
<p>以降が，Q-learningの主な実装となります．実装の内容は<a class="reference internal" href="#q-learning">Algorithm 4.2</a>とほぼ同じ内容になっています．</p>
<p>ただし，この環境には，終了状態が存在します．そして，終了状態以後は報酬が入らないため，<span class="math notranslate nohighlight">\(s\)</span>が終了状態だと<span class="math notranslate nohighlight">\(Q(s,a) = 0, \forall a \in \mathcal{A}\)</span>となります．
シミュレーション環境においては，次の状態<code class="docutils literal notranslate"><span class="pre">next_state</span></code>が終了状態であると，終了状態フラグ<code class="docutils literal notranslate"><span class="pre">done</span></code>が<code class="docutils literal notranslate"><span class="pre">True(=1)</span></code>となるため，<span class="math notranslate nohighlight">\(q\)</span>の更新式は，</p>
<div class="math notranslate nohighlight">
\[\begin{split}q(S_t, A_t) &amp;\leftarrow q(S_t, A_t) + \alpha(n(S_t, A_t))( R_t + \gamma (1 - \text{done}) \max_{a' \in \mathcal{A}} q(S_{t+1}, a') - q(S_t, A_t))\\
  q(s, a) &amp;\leftarrow q(s, a), \ s \neq S_t, a \neq A_t\end{split}\]</div>
<p>となることに注意してください．</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">performance_log</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># パフォーマンスのログ</span>

<span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>

<span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100000</span><span class="p">):</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">policy_eps_greedy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">EPSILON</span><span class="p">)</span> <span class="c1"># 行動を選択</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c1"># 行動を実行，次の状態/報酬/終了判定を得る</span>
    <span class="n">n</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># 状態行動対への訪問回数</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">n</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>  <span class="c1"># ステップサイズ</span>
    <span class="n">q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span>
        <span class="n">reward</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">q</span><span class="p">[</span><span class="n">next_state</span><span class="p">,</span> <span class="p">:])</span> <span class="o">-</span> <span class="n">q</span><span class="p">[</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
    <span class="p">)</span> <span class="c1"># qを更新</span>

    <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># 100ステップおきにパフォーマンスを測定</span>
        <span class="n">performance_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">check_performance</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">q</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

<span class="c1"># ビジュアライズ</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100000</span><span class="p">,</span><span class="mi">100</span><span class="p">)],</span> <span class="n">performance_log</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;step&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/04q-learning_10_0.png" src="../_images/04q-learning_10_0.png" />
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./day1"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="03bellman.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">3. </span>ベルマン方程式</p>
        </div>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      著者 Yukinari Hisaki<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>