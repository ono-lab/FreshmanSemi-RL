{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 価値関数\n",
    "\n",
    "```{math}\n",
    "\\newcommand{\\E}{{\\mathrm E}}\n",
    "\\newcommand{\\underE}[2]{\\underset{\\begin{subarray}{c}#1 \\end{subarray}}{\\E}\\left[ #2 \\right]}\n",
    "\\newcommand{\\Epi}[1]{\\underset{\\begin{subarray}{c}\\tau \\sim \\pi \\end{subarray}}{\\E}\\left[ #1 \\right]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章では，強化学習理論の基礎となる，価値関数/行動価値関数というものについて見ていきます．価値関数/行動価値関数とは直感的に捉えると，ある状態$s$や状態行動対$(s,a)$が，どのくらいの価値を持つかを表すものです．価値関数は強化学習アルゴリズムの目的である\"最適方策\"と密接な関わりがあり，重要な概念です．ここでは，具体的に以下の4点について解説していきます．\n",
    "\n",
    "- 価値関数/行動価値関数の定義\n",
    "- これらを用いた最適方策の定義\n",
    "- 価値関数の性質\n",
    "- 価値関数の推定\n",
    "\n",
    "## 定義\n",
    "\n",
    "まずは，一般的に用いられる価値関数/行動価値関数の定義を確認します．\n",
    "\n",
    "- 価値関数(Value Function)\n",
    "\n",
    "  $$V^\\pi(s) = \\underE{\\pi}{\\lim_{T \\rightarrow \\infty} \\sum_{t=0}^T \\gamma^t R_t | S_0 = s}$$\n",
    "\n",
    "- 行動価値関数(Action Value Function)\n",
    "\n",
    "  $$Q^\\pi(s,a) = \\underE{\\pi}{\\lim_{T \\rightarrow \\infty} \\sum_{t=0}^T \\gamma^t R_t | S_0 = s, A_0 = a}$$\n",
    "\n",
    "価値関数$V^\\pi(s)$は「ある状態$s$から方策$\\pi$に従って行動した場合の得られる期待割引累積報酬」と解釈できます．\n",
    "また，行動価値関数は「ある状態$s$で，ある行動$a$を取った後，方策$\\pi$に従って行動した場合の得られる期待割引累積報酬」と解釈できます．\n",
    "２つの違いは，価値関数$V^\\pi(s)$は最初の行動$A_0$が方策に従ってサンプルされるのに対して，行動価値関数$Q^\\pi(s,a)$では$A_0$が引数によって与えられるという点です．なので，2つの関数には以下の関係が成り立ちます．\n",
    "\n",
    "$$V^\\pi(s) = \\underE{a \\sim \\pi}{Q^\\pi(s, a)}$$\n",
    "\n",
    "## 価値関数/行動価値関数の推定(モンテカルロ法)\n",
    "\n",
    "価値関数/行動価値関数の定義から，\n",
    "\n",
    "  1. 確率変数である$\\sum \\gamma^t R_t$を実際にサンプルする．\n",
    "  2. サンプルされたデータを元に，$V^\\pi(s), Q^\\pi(s, a)$を推定する．\n",
    "\n",
    "という非常にシンプルな推定方法を考えることが出来ます．\n",
    "このようなコンセプトを持った手法はモンテカルロ法と言われています．\n",
    "\n",
    "実際にモンテカルロ法をシミュレーションに適用する様子を見ていきましょう．\n",
    "\n",
    "### 実験設定\n",
    "\n",
    "環境として，OpenAI Gymの[FrozenLake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)を用います．\n",
    "環境に関する設定は，ドキュメントを参照してください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\n",
    "    \"FrozenLake-v1\",\n",
    "    desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"],\n",
    "    map_name=\"4x4\",\n",
    "    is_slippery=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "価値推定の対象となる方策は，ランダム方策とします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    return env.action_space.sample()  # 行動を行動空間からランダムにサンプルする\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に，各パラメータを設定します．\n",
    "環境から状態数と行動数を取得し，割引率$\\gamma = 0.99$と設定します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "GAMMA = 0.99\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "環境に対して，方策を用いて，経験を集める関数を定義します．\n",
    "ここで経験とは，\n",
    "\n",
    "$$[(s_0, a_0, s_{t+1})]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    state: int\n",
    "    next_state: int\n",
    "    action: int\n",
    "    reward: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def generate_episode(env: gym.Env, policy: Callable):\n",
    "    episode: list[Transition] = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = policy(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode.append(Transition(state, next_state, action, reward))\n",
    "        if done:\n",
    "            break\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "    return episode\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97504645a1f23459c17c212274a0bcf9c19874fbce259200e47a4c8c54b7985d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
